<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science with Big Dillinger</title>
    <description>Thoughts about Data Science, R and Python tutorials, and life as a Statistics graduate student at Northwestern
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 13 Sep 2017 23:40:05 -0500</pubDate>
    <lastBuildDate>Wed, 13 Sep 2017 23:40:05 -0500</lastBuildDate>
    <generator>Jekyll v3.4.5</generator>
    
      <item>
        <title>The Case for Big Brother in Big Data.</title>
        <description>&lt;h3 id=&quot;garbage-in-garbage-out&quot;&gt;Garbage in, garbage out&lt;/h3&gt;

&lt;p&gt;Data Scientists usually wear the messiness of the data they work with as a badge of honor. And they should! It’s not easy to take just any dataset and shape it into something that you can build a predictive model out of, or even just craft some sort of executive-level quantitative report. But when the task is training a model to deploy on real-time data, Data Scientists need to make sure their data was intended for more than a one-off type of analysis. We should demand structure.&lt;/p&gt;

&lt;p&gt;Here is an impression of a dataset a Data Scientist might see in the wild -&lt;/p&gt;

&lt;table&gt;
  &lt;caption&gt;Made up lawnmower data. Can anyone else relate to this?&lt;/caption&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ser_no&lt;/th&gt;
      &lt;th&gt;last_updated&lt;/th&gt;
      &lt;th&gt;trbd&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;U542&lt;/td&gt;
      &lt;td&gt;1900/01/01 00:00:99&lt;/td&gt;
      &lt;td&gt;K1242&lt;/td&gt;
      &lt;td&gt;245.88&lt;/td&gt;
      &lt;td&gt;342198&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;KT294&lt;/td&gt;
      &lt;td&gt;2017-09-10&lt;/td&gt;
      &lt;td&gt;K1242&lt;/td&gt;
      &lt;td&gt;232.98&lt;/td&gt;
      &lt;td&gt;342198&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;hello&lt;/td&gt;
      &lt;td&gt;NULL&lt;/td&gt;
      &lt;td&gt;test -- hold for Jim&lt;/td&gt;
      &lt;td&gt;112.23&lt;/td&gt;
      &lt;td&gt;342198&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;NULL&lt;/td&gt;
      &lt;td&gt;yes&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;342198&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;serial_no_234&lt;/td&gt;
      &lt;td&gt;Tuesday&lt;/td&gt;
      &lt;td&gt;KT234&lt;/td&gt;
      &lt;td&gt;112.23&lt;/td&gt;
      &lt;td&gt;342198&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Maybe the people who supplied you this lawnmower data don’t know what the &lt;em&gt;trbd&lt;/em&gt; field means, because their DBA who quit in 2011 designed their Salesforce database. Maybe your manager wants you to use this to build a model to predict lawnmower prices based on weather patterns, but nobody can tell you if &lt;code class=&quot;highlighter-rouge&quot;&gt;last_updated&lt;/code&gt; can safely be interpreted as when each lawnmower was sold, which is pretty crucial if you’re going to try to join historical weather conditions to this price data.&lt;/p&gt;

&lt;p&gt;My point is that unstructured, free-text data can quickly lead to the “garbage in, garbage out” phenomena.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://alison.dbsdataprojects.com/wp-content/uploads/sites/82/2016/04/cartoon-metadata.png&quot; alt=&quot;Dilbert&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;For supervised learning models, we can only expect a model to perform as well as quality of the labeled data used for training. If we don’t have a clue what separates a cat from a dog sample, your model is doomed, because evaluation metrics are now meaningless. More humans can help. Humans can make heuristics to label our data. We can use Amazon Turk to get many humans to help label our data, and humans can send data to other (human) subject matter experts for label verification. But does that not defeat the entire notion of machine learning, that data needs to pass under the nose of a human before we can learn anything interesting from it?&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;big-government-is-your-friend&quot;&gt;Big government is your friend&lt;/h3&gt;

&lt;p&gt;We should demand more from our data, and the government can help demand it. Take the electrical grid. The Federal Engery Regulatory Commission (FERC) has an immense set of guidelines for electricity producers and transmission owners when it comes to reporting as how much energy was produced and transmitted across the grid. For example, FERC requires that all electric utility companies (e.g. ones that power entire towns, cities, or regions) have to complete the (Energy Information Administration’s Form 906)[https://www.eia.gov/electricity/2008forms/906-923crosswalk.xls] whereby the utilities have to outline exactly how much of what types of fuels each electricity plant used over the last year. Even the units of fuel are prespecified to a selection of either “tons,” “barrels,” or “thousands of cubic feet.” Now, Data Scientists won’t need to guess the units used, or, god forbid, call the power plant and find the person who filled out the EIA 906 form to ask them about units. FERC has enforced reasonable standards in our public data, and this benefits Data Scientists by allowing us to build models that can leverage the EIA 906 form for any major electrical plant in the country, not just the handful that would have included units of input energy without the data guidelines.&lt;/p&gt;

&lt;table&gt;
  &lt;caption&gt;EIA 906 data for electricity utilties. A Data Scientist's dream.&lt;/caption&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;PLANT NAME&lt;/th&gt;
      &lt;th&gt;PLANT ID&lt;/th&gt;
      &lt;th&gt;STATE&lt;/th&gt;
      &lt;th&gt;PRIME MOVER TYPE&lt;/th&gt;
      &lt;th&gt;ENERGY SOURCE&lt;/th&gt;
      &lt;th&gt;GROSS GENERATION (Mwh)&lt;/th&gt;
      &lt;th&gt;NET GENERATION (Mwh)&lt;/th&gt;
      &lt;th&gt;UNITS (Tons, Barrels, or 1000s tons)&lt;/th&gt;
      &lt;th&gt;STOCKS AT END OF PERIOD&lt;/th&gt;
      &lt;th&gt;HEAT CONTENT PER UNIT OF FUEL (Million Btu)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Sunapee&lt;/td&gt;
      &lt;td&gt;AP123&lt;/td&gt;
      &lt;td&gt;HIe&lt;/td&gt;
      &lt;td&gt;Diesel engine&lt;/td&gt;
      &lt;td&gt;Diesel fuel&lt;/td&gt;
      &lt;td&gt;100.1&lt;/td&gt;
      &lt;td&gt;12.5&lt;/td&gt;
      &lt;td&gt;Barrels&lt;/td&gt;
      &lt;td&gt;34.1&lt;/td&gt;
      &lt;td&gt;6832.22&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The government has solved so many other huge data headaches simply by enforcing similar data schemas. What if any publicly-traded company was able to report their annual earnings using a report template they created themselves? Apple would use one set of profit reporting guidelines while Ford used another and IBM used another one, and none of the reports would look the same. Instead, the Securities and Exchange Commission makes everyone use a 10-k form. What if there were no standards for reporting daily trading volume, high, low, and closing stock prices? We all benefit immensely from having agreed-upon and enforced data schemas.&lt;/p&gt;

&lt;p&gt;Take just the example of a Vehicle Identificaion Number (VIN). All cars have one, and it’s a standard enforced by the International Organization for Standardization - a body run by the UN, the definition of “big government.” Any Data Scientist should immediately appreciate the VIN - it’s an immutable, easily-interpreted primary key. If you have access to a car’s VIN, you have a unique identifier for any car in addition to information about the car’s characteristics that is vendor-agnostic. Without some sort of ISO convention for unique automobiles, any type of cross-vehicle make comparison would be a nightmare of ad hoc rules about what constitues an SUV, or fixing discrepancies between auto manufacturers that track the age of each car based on the year it came off of the assembly line versus manufacturers that track age based on a car’s model year.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-point&quot;&gt;…the point?&lt;/h3&gt;

&lt;p&gt;I only say all of this for two reasons - (1) to defend the notion that the government does do useful things and (2) because I believe that the industries most ripe for distruption by the Internet of Things (IoT) analytics are those that are in some way overseen by some type of governmental or regulatory body. Left to their own devices, groups of competing entities building things like sensors, valves, power plants, or automobiles would most likely just create their own data and reporting conventions. This creates obstacles in letting fully human-independent machine learning take root in a way that easily scales across an industry. When “big brother” is around to nudge everyone to create, at the very least, data that vaguely resembles everyone else’s data, this unlocks doors in terms of the scope of problems upon which Data Scientists can hope to deploy machine learning solutions.&lt;/p&gt;

</description>
        <pubDate>Wed, 13 Sep 2017 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/blog/data,/thought/pieces/2017/09/13/case_for_big_govt.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/data,/thought/pieces/2017/09/13/case_for_big_govt.html</guid>
        
        
        <category>blog</category>
        
        <category>data,</category>
        
        <category>thought</category>
        
        <category>pieces</category>
        
      </item>
    
      <item>
        <title>Meet the Greebles. In pursuit of alternative deep learning tutorial set.</title>
        <description>&lt;h3 id=&quot;tired-of-mnist&quot;&gt;Tired of MNIST?&lt;/h3&gt;

&lt;p&gt;Most aspiring data scientists have at least heard of neural networks and “deep learning” by now, and you’ve probably noticed that deep learning has been getting really high-profile coverage, especially with the advent of self-driving cars and Google DeepMind’s AlphaGo program that &lt;a href=&quot;https://deepmind.com/alpha-go.html&quot;&gt;beat a professional Go player&lt;/a&gt; in March, 2016.&lt;/p&gt;

&lt;p&gt;Not wanting to miss out on the trend, I began reading blog posts, tutorials, and doc pages on the mathematical foundations of different types of neural networks, and of course, how the hell to implement one. I definitely suggest Andrew Ng’s Coursera course, &lt;a href=&quot;https://www.coursera.org/learn/machine-learning&quot;&gt;“Machine Learning”&lt;/a&gt; for an in-depth treatment of multi-layer perception neural networks. Most of the bleeding-edge neural network programming frameworks available to your typical data scientist are, not surprisingly, in Python. Here are a few of the deep learning tools I’ve read about:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://keras.io/&quot;&gt;Keras&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://deeplearning.net/software/theano&quot;&gt;Theano&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Lasagne/Lasagne&quot;&gt;Lasagne&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dnouri/nolearn&quot;&gt;nolearn&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These offerings range from full-scale Python libraries that allow you to write every aspect of a deep net from scratch (e.g. Theano) to smaller libraries (e.g. nolearn)that offer wrappers to deep learning backends like Theano and TensorFlow. R hosts many neural network packages too, but since so much of deep learning is focused on GPU computing (using graphical processing units to handle model training), most of the DIY deep learning world is centered in Python.&lt;/p&gt;

&lt;p&gt;Deep learning is a huge field, and I’ve just begun to scratch the surface. Fortunately, there are dozens, probably hundreds of tutorials out there that will give you the illusion of letting you feel like you know what you’re doing. Basically all of the Python offerings are open source, so you’ll naturally be invited to a host of different tutorials, which is ten thousand times A GREAT THING. The only downside is, you’ll be classifying the MNIST numbers over and over.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/MNIST_database&quot;&gt;MNIST&lt;/a&gt; numbers is a fully labeled set of 60,000 training and 10,000 test images, the images being handwritten digits, 0 through 9. Almost every intro to deep learning tutorial out there revolves around training a neural network on the training set and then minimizing the misclassification error of the test set. Very quickly you can have a model that only misclassifies about 33/10,000 digits, and the misclassified digits were poorly written in the first place:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FrankFineis/FrankFineis.github.io/master/images/misclassified_mnist_pics.png&quot; class=&quot;inline&quot; width=&quot;600px&quot; height=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While the MNIST set is big enough to merit GPU computing, I’m more interested in learning about neural network architectures first, not so much GPU computing. Thus, a set of 60,000 images pertaining to 10 separate classes is over-kill for me. I want to run a homemade example.&lt;/p&gt;

&lt;h3 id=&quot;why-deep-learning&quot;&gt;Why deep learning?&lt;/h3&gt;

&lt;p&gt;The MNIST numbers are a great resource, no doubt, but these tutorials centered on classifying the MNIST set can easily mask the complexity and patietence required to build a high-perforant deep network. Jumping in with an MNIST tutorial might make you forget why we’re even using neural networks in the beginning.&lt;/p&gt;

&lt;p&gt;The underlying reason why we use a neural net with images (either multi-layer perceptron networks, convolutional neural nets, or maybe even autoencoders) is that they let us learn really good image bases in an unsupervised way. That’s the goal in most of machine learning, deriving bases (simple representations, the LEGOS, if you will) that comprise the objects in your training set. For example, here are some of the bases learned for the MNIST training set while training a multi-layer perceptron model:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/FrankFineis/FrankFineis.github.io/master/images/mnist_weights.png&quot; class=&quot;inline&quot; width=&quot;400px&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;That’s it. I mean, that’s the biggest reason - deep nets learn really good bases that aren’t confined to being linear combinations of the original pixel features (like PCA), or convex combinations of pixel features (like archetypal analysis), but seemingly arbitrarily complex, non-linear transformations of pixel features in the training set.&lt;/p&gt;

&lt;h3 id=&quot;outstanding-questions-that-i-have-about-deep-learning&quot;&gt;Outstanding questions that I have about deep learning:&lt;/h3&gt;

&lt;p&gt;Many things about deep learning go unaddressed in a typical walkthrough to a new deep learning framework like a Keras or a Theano. Here are a few questions I’ll hope to explore on my own that I feel are important to understanding deep learning as a tool and not a solution.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;What are the dimensionality constraints for training a multi-layer perception model? Is it even possible to train a model using less than a few thousand training examples? Is there a ratio of &lt;strong&gt;n&lt;/strong&gt; (training examples) to &lt;strong&gt;p&lt;/strong&gt; (feature dimension) beyond which an MLP or a convolutional neural network would be particularly easy to train?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Is there a way to assess statistical significance using a neural net - e.g., can I get something like a standard error for the coefficients in a weights matrix between two layers of an MLP?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Besides bootstrapping or cross-validation, can I get some sort of confidence interval around the predictions coming out of, say, a deep net used for multi-class classification?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;meet-the-greebles&quot;&gt;Meet the Greebles&lt;/h3&gt;

&lt;p&gt;Greebles look exactly how they sound, as in, they’re purple minions with weird phallic horns:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://classconnection.s3.amazonaws.com/261/flashcards/472261/png/greebles.png&quot; class=&quot;inline&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;“Male” greebles have concave-up horns and “female” greebles have concave-down horns; they’re commonly used in psychological studies related to facial and object recognition, and according to Wikipedia they’re commonly found in psychology textbooks. In my &lt;a href=&quot;https://github.com/fineiskid/Greeble_image_learning&quot;&gt;greebles repo&lt;/a&gt;, you’ll find 160 color greeble images with 84 males and 76 females that I got from Carnegie Mellon’s &lt;a href=&quot;http://wiki.cnbc.cmu.edu/Novel_Objects&quot;&gt;TarrLab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the coming tutorials, I’m going to see how well I can train a host of models, including a couple of deep-nets, to classify greebles by “gender”. There is a “greeble generator” available through the TarrLab too, should we need a bigger set for training. But overall, I’m lucky to just have such a solid set of labeled, uniformly positioned, shaped, and colored images, all of which make the MNIST set such a good resource.&lt;/p&gt;

&lt;h3 id=&quot;what-ill-explore-with-greebles&quot;&gt;What I’ll explore with greebles&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Greeble decompositions:&lt;/strong&gt; PCA “eigengreeble” decomposition, non-negative matrix factorization, archetypal/prototypical analyses&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Logistic and kernel-based&lt;/strong&gt; classification methods&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multi-layer perceptron&lt;/strong&gt; neural nets&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Convolutional&lt;/strong&gt; neural nets&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Autoencoders:&lt;/strong&gt; similar to PCA, the autoencoder finds a low-rank approximation to an input&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;greeble-decomposition-tutorial&quot;&gt;Greeble decomposition &lt;a href=&quot;https://github.com/fineiskid/Greeble_image_learning/blob/master/greeble_decompositions.ipynb&quot;&gt;tutorial&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;To get started, check out the &lt;a href=&quot;https://github.com/fineiskid/Greeble_image_learning/blob/master/greeble_decompositions.ipynb&quot;&gt;tutorial&lt;/a&gt; I’ve posted in my greebles repository for how to get started loading the greeble images, and how to start reducing the dimension of the initial feature space. Each greeble .tif file is 360 pixels by 320 pixels, so we have 160 greebles with each 115200 interesting features. No learning algorithm is going to be able to swim in such a high-dimensional situation, so I’ve used principal component analysis and non-negative matrix factorization as ways to reduce the dimension of our greeble set prior to fitting a classification model.&lt;/p&gt;

</description>
        <pubDate>Wed, 01 Jun 2016 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/blog/2016/06/01/Greeble_decomposition.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2016/06/01/Greeble_decomposition.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>tl;dr Intro to Bayesian Statistics</title>
        <description>&lt;h3 id=&quot;youve-heard-of-the-debate&quot;&gt;You’ve heard of the debate&lt;/h3&gt;
&lt;p&gt;All data scientists have heard of the famous &lt;em&gt;frequentist-Bayesian controversy&lt;/em&gt;.
If you’re like me, when you’re asked the question, “Bayesian or a frequentist?”
you just say “frequentist” because you’re just guessing that Bayesian statistics
has something to do with Bayes’ Theorem, and you haven’t used Bayes’ Theorem
since college when you had the &lt;a href=&quot;https://en.wikipedia.org/wiki/Monty_Hall_problem&quot;&gt;Monty-Hall
problem&lt;/a&gt; on your homework. Am
I alone? Quite possibly.&lt;/p&gt;

&lt;p&gt;Anyways, the other day I finally came across an excellent, (potentially) real-
world situation that clearly illustrates the difference between a Bayesian
outlook and the frequentist outlook.&lt;/p&gt;

&lt;h3 id=&quot;the-set-up&quot;&gt;The set-up&lt;/h3&gt;
&lt;p&gt;Just in case you’re a lawyer, this example is entirely fictional and is taken
directly from &lt;em&gt;Bayesian Methods for Data Analysis&lt;/em&gt; by Carlin and Louis, and all
characters and companies are fictious, any seeming correspondence to actual
people and companies is purely a coincidence. Here’s the set-up: it’s November
2015, just a few weeks before Shmerger King starts wants to start selling
hotdogs. You’re a hotshot consultant hired to test whether or not people prefer
hotdogs made out of mice (&lt;strong&gt;case A&lt;/strong&gt;) or hotdogs made out of cats (&lt;strong&gt;case B&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;Because of your time constraint, you were only able to (randomly) gather 25
people to sign up for your study. Here’s your study: each person gets two
hotdogs, one made out of mice and one made out of cats (the subjects don’t know
which hotdog is which), and you need to assess the probability that people
genuinely prefer mice hotdogs (case A), and we’re going to call this &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.
You ran the experiment and 20 people said they prefer mice hotdogs. So… what
do you tell Shmerger King? What if this 80% mice hotdog preference was a fluke?&lt;/p&gt;

&lt;h3 id=&quot;bayes-frequentist-debate-summed-up&quot;&gt;Bayes-frequentist debate summed up&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Frequentists&lt;/strong&gt; will say this: &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is the average fraction of people that
prefer mice hotdogs to cat hotdogs if you ran the experiment infinity times. If
not infinity times, then at least like, a bunch of times. There’s not enough
time to repeat this experiment over and over hotshot!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bayesians&lt;/strong&gt; will say this:
&lt;script type=&quot;math/tex&quot;&gt;\text{Pr}(\theta|data) \propto \text{likelihood}(data|\theta)\times\text{Pr}(\theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Bayesians call &lt;script type=&quot;math/tex&quot;&gt;\text{Pr}(\theta | data)&lt;/script&gt; the &lt;strong&gt;posterior probability&lt;/strong&gt; of
&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; . Now, it’s up to you, the hotshot consultant, to postulate both a
&lt;em&gt;likelihood&lt;/em&gt; function that specifies the probability of the data given some
value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and a &lt;em&gt;prior&lt;/em&gt; distribution for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; itself.&lt;/p&gt;

&lt;h3 id=&quot;not-that-bad&quot;&gt;Not that bad&lt;/h3&gt;
&lt;p&gt;Yeah, ok, Bayesians have to specify the likelihood function of the data and the
prior for the missing parameter. Frequentists don’t do this. But for this
experiment, specifying a likelihood is pretty easy: Clearly, each subject’s
response should be independent from each of the other 24 subjects’ response, and
since there are just two outcomes, “mice hotdog preference” or “cat hotdog
preference,” the likelihood really lends itself to a binomial distribution. This
is to say, if &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; takes on some value, say 0.75, then the probability that
20/25 people will prefer the mice hotdogs is a lot higher than if &lt;script type=&quot;math/tex&quot;&gt;\theta =
0.3&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Priors are a little bit more complicated, because as a hotshot consultant you
have to think about &lt;em&gt;what you genuinely believe&lt;/em&gt; about the distribution of
&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Maybe you think that people will really like the mice hotdogs because
there’s some literature out there about people eating mice. Maybe you have zero
information about people’s preferences. Maybe you’re skeptical and you think
that people won’t be able to tell the difference between the hotdogs, suggesting
that &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is probably centered around 0.5. A good pick for a prior
distribution would be the beta distribution because it’s really malleable, and
it’s actually a &lt;a href=&quot;https://en.wikipedia.org/wiki/Conjugate_prior&quot;&gt;conjugate prior&lt;/a&gt;
to the binomial distribution. More on conjugate priors below.&lt;/p&gt;

&lt;h3 id=&quot;envisioning-likelihood&quot;&gt;Envisioning likelihood&lt;/h3&gt;

&lt;p&gt;The likelihood function defines the probability of observing the data (20 mice
hotdog preferences in 25 trials) under a particular value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. Below, we
can see that if &lt;script type=&quot;math/tex&quot;&gt;\theta = 0.8&lt;/script&gt; (the frequentist estimate), there’s a much higher
probability that we see 20/25 mice hotdog preferences as opposed to when &lt;script type=&quot;math/tex&quot;&gt;\theta
= 0.3&lt;/script&gt;. If we kept going down this path, we’d probably just pick &lt;script type=&quot;math/tex&quot;&gt;\theta = 0.8&lt;/script&gt;
because this is the parameter estimate that &lt;em&gt;maximizes likelihood&lt;/em&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#Note that binomial distribution is discrete.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pmf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;theta = 0.8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pmf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;theta = 0.3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Likelihood under different thetas&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Population ratio of (Mice preferences)/(Cat preferences)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Density&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FrankFineis/FrankFineis.github.io/master/images/burgers_likelihood_curves.png&quot; class=&quot;inline&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;envisioning-the-prior&quot;&gt;Envisioning the prior&lt;/h3&gt;

&lt;p&gt;The prior distribution for &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; allows you, the hotshot consultant, to
inject your underlying believe about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, which is the proportion of all of
Shmurger King’s clientelle that really prefers mice hotdogs to cat hotdogs, the
thing you’re trying to pin down.&lt;/p&gt;

&lt;p&gt;The beta distribution is a natural fit for the prior here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;its support (the domain of values you can plug into the beta probability
distribution function) is [0,1]&lt;/li&gt;
  &lt;li&gt;Through its two parameters, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; (in Bayesian statistics, the
parameters of your prior are called &lt;em&gt;hyperparameters&lt;/em&gt;, the shape of the beta
distribution is highly configurable&lt;/li&gt;
  &lt;li&gt;It’s the conjugate prior distribution to the binomial distribution (so, our
likelihood function), which means that the posterior distribution (i.e.
&lt;script type=&quot;math/tex&quot;&gt;Pr(\theta | data)&lt;/script&gt;) is also a beta distribution, just with different parameters
than the prior. That’s what &lt;em&gt;conjugacy to the likelihood&lt;/em&gt; means, that your
posterior distribution is of the same type as your prior.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;what-do-you-think&quot;&gt;What do you think?&lt;/h4&gt;
&lt;p&gt;Bayesian statistics lets us inject our subjective beliefs into our models.
Perhaps, you think one of the following statements is true:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1) You have absolutely no idea whether or not people prefer mice hotdogs over
cat hotdogs&lt;/li&gt;
  &lt;li&gt;(2) You think it’s likely that people won’t be able to tell the difference, so
&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; tends to be around 0.5&lt;/li&gt;
  &lt;li&gt;(3) You think that people will either really love or really hate mice hotdogs,
so &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; will likely take on extreme values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With the beta(1,1), beta(0.5, 0.5), and beta(2,2) distributions, respectively,
we can emulate those scenarios:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Define vector of potential theta values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Plot prior distributions&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;beta(1,1)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;beta(0.5,0.5)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;DarkOrange&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;beta(2,2)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Different prior parameterizations&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Theta&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Density&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FrankFineis/FrankFineis.github.io/master/images/burgers_priors.png&quot; class=&quot;inline&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;putting-it-together-the-posterior&quot;&gt;Putting it together: the posterior&lt;/h3&gt;

&lt;p&gt;Since the beta distribution is the conjugate prior to the binomial likelihood
function (no derivation here), the posterior is also a beta distribution, only
now \[\alpha = \alpha_{prior} + \text{successes}\] and \[\beta =\beta_{prior} + n -
\text{successes}\]
The derivation of the posterior’s parameters involves multiplying the beta prior and binomial likelihood functions together,
it’s a big drawn out process. As a hotshot consultant, you’d probably just want
to consult mathematica, Google, or the &lt;em&gt;Bayesian Methods for Data Analysis&lt;/em&gt;
book. The main point is that the posterior distribution is just a reparameterization of the prior! That’s why people like conjugate priors.&lt;/p&gt;

&lt;p&gt;Anyway, the main point is that our choice of hyperparameters (i.e. how we
parameterized the prior), will affect the posterior distribution. In plotting
the resultant posterior distributions of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, notice that your choice for
hyperparameters really doesn’t change the shape or location of the posterior!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Observed data: number of successes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Plot posteriors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;one_postr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;five_postr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;two_postr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_postr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Posterior w/ beta(1,1) prior&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;five_postr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Posterior w/ beta(0.5,0.5) prior&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;two_postr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;DarkOrange&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Posterior w/ beta(2,2) prior&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Posterior distributions under different prior parameterizations&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Theta&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Density&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FrankFineis/FrankFineis.github.io/master/images/burgers_posteriors.png&quot; class=&quot;inline&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;drawing-conclusions&quot;&gt;Drawing conclusions&lt;/h3&gt;
&lt;p&gt;As you can see from the plots directly above, the posterior distribution’s mode
settles right around &lt;script type=&quot;math/tex&quot;&gt;\theta = 0.8&lt;/script&gt; across our three prior parameterizations.
One of the benefits of a Bayesian analysis is that there’s no ambiguity in what
we mean by “confidence interval.”&lt;/p&gt;

&lt;p&gt;Remember the hoopla in your intro to stats class about confidence intervals? To
frequentists, the term “confidence interval” means that “If I ran the experiment
100 times, about 95 of the confidence intervals that I estimate (I’m estimating
the 95% confidence interval of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; in each experiment) will cover the true
value of &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.” It’s way easier in Bayesian statistics.&lt;/p&gt;

&lt;p&gt;Now, you can finally say, “The probability that &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; lies between a and b is
95%,” and Bayesian statisticians call “confidence intervals” &lt;strong&gt;“credible sets.”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All you do to get a “credible set” is find differences in cumulative densities
of the posterior! Moreover, we can use the beta cumulative density function to
assess the probability that &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is greater than some value that the
Schmerger King execs will be happy with, such as &lt;script type=&quot;math/tex&quot;&gt;\theta = 0.6&lt;/script&gt;, under each
prior distribution:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;## Find posterior modes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;posteriors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_postr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;five_postr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;two_postr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;modes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_along_axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posteriors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Calculate credible intervals&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;posterior_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;CI&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;posterior_params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;CI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posterior_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posterior_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posterior_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;posterior_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Under prior distribution {} we are {:.2f}&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;% &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sure that theta &amp;gt; 0.6.&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FrankFineis/FrankFineis.github.io/master/images/tldrbayes_CDF_results.png&quot; class=&quot;inline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given that we’ve made some pretty reasonable assumptions for the likelihood and
prior distributions, you can safely report that people firmly prefer mice
hotdogs to cat hotdogs.&lt;/p&gt;
</description>
        <pubDate>Thu, 05 May 2016 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/blog/2016/05/05/tldrBayes.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2016/05/05/tldrBayes.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Installing OpenCV on Mac OS X in an Anaconda Virtual Environment</title>
        <description>&lt;h3 id=&quot;whats-opencv&quot;&gt;What’s OpenCV?&lt;/h3&gt;
&lt;p&gt;Ahhh, computer vision, such a cool field! Lately, I’ve been trying to become more knowledgeable about CV and image processing in python. &lt;a href=&quot;http://opencv.org/downloads.html&quot;&gt;OpenCV&lt;/a&gt; (CV = ‘computer vision’) is an excellent open source computer vision software library written in C++ that supports C++, C, Python, Java, and Matlab API’s. OpenCV will supply you with functions that will let you detect faces in images, track objects in a video, and perform any number of image processing tasks.&lt;/p&gt;

&lt;p&gt;The only problem is: how the &lt;em&gt;hell&lt;/em&gt; do I install OpenCV so that I can use it in conjunction with a Jupyter notebook? Let’s be honest, most likely you’re either you’re using a Jupyter notebook, Spyder, or the ipython terminal (if you’re a real sadist) to test your python code. And especially if you’re coding for image processing, you’re going to want to view your progress without having (a) a million separate images open and (b) having to wait for Spyder to inevitably crash. That’s the beauty of a Jupyter notebook - when you’re using it with &lt;a href=&quot;http://matplotlib.org/&quot;&gt;Matplotlib&lt;/a&gt;, you can just display your images and videos in a living document!&lt;/p&gt;

&lt;p&gt;For me, my ideal OpenCV situation would be for me to be able to simply type and evaluate the following &lt;code class=&quot;highlighter-rouge&quot;&gt;import&lt;/code&gt; statements with zero errors or package conficts:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;opencv&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;The_Cure_Rules.png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;problems-with-traditional-installation-methods&quot;&gt;Problems with traditional installation methods&lt;/h3&gt;

&lt;p&gt;There are &lt;strong&gt;many&lt;/strong&gt; ways to install OpenCV. The standard approach is to download it from the OpenCV website and then compile and install OpenCV using the software building utility “CMake” all within a virutal Python environment. I’ve gone down this route according to &lt;a href=&quot;http://www.pyimagesearch.com/2015/06/15/install-opencv-3-0-and-python-2-7-on-osx/&quot;&gt;Adrian Rosebrock&lt;/a&gt;’s fabulous installation walkthrough, and if you just want to have access to OpenCV 3.0, I suggest you consider it. But, at the end of the day, there are even more steps required after Adrian’s 9 steps to get OpenCV compatible with a Jupyter notebook. Other installation walkthroughs I’ve found tend to be generally convoluted and assume that you have Homebrew, XCode, maybe MacPorts, or just experience in general with installing and building software packages. &lt;strong&gt;Wouldn’t it be great if we could just run something analogous to &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install opencv&lt;/code&gt;?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you’re like me (maybe you’re not) I often think that &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install&lt;/code&gt;‘ing a Python package is the same thing as R’s &lt;code class=&quot;highlighter-rouge&quot;&gt;install.packages&lt;/code&gt; function - while we get similar functionality, R packages come with the luxury of basically never interfering with other R package dependencies! If one package needs a newer or older version of some other package you’ve already installed, &lt;code class=&quot;highlighter-rouge&quot;&gt;install.packages&lt;/code&gt; will most likely just take care of everything for you. Python packages, on the other hand, will often have dependencies on specific versions of other packages, so if you &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install&lt;/code&gt; one package, other package may fail to import because their dependent packages have been updated. That’s why we use virtual environments; my favorite method for creating and running virtual environments is with &lt;a href=&quot;https://www.continuum.io/downloads&quot;&gt;Anaconda&lt;/a&gt;, a Python distribution that comes with Sklearn, Scipy, NumPy, Jupyter notebook, and most of the other essential tools a data scientist needs when using Python.&lt;/p&gt;

&lt;p&gt;Overall, I installed OpenCV cleanly in just a few steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Install Anaconda, make Anaconda’s Python your system’s default Python (skip if you already have this).&lt;/li&gt;
  &lt;li&gt;Create a virtual environment.&lt;/li&gt;
  &lt;li&gt;Make sure all Conda packages are up-to-date.&lt;/li&gt;
  &lt;li&gt;Run &lt;code class=&quot;highlighter-rouge&quot;&gt;conda install -c https://conda.binstar.org/menpo opencv&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Test.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-install-anaconda-skip-if-you-already-have-anaconda&quot;&gt;(1) Install Anaconda. (Skip if you already have Anaconda).&lt;/h3&gt;

&lt;p&gt;First off, I’m still a python 2 guy. Yeah, there’s python 3, but I grew up on Py 2.7 and it’ll take a lot to pry it from my cold, dead hands. So I have a python 2.7 Anaconda environment running on my computer. Your choice.&lt;/p&gt;

&lt;p&gt;I went to the Anaconda &lt;a href=&quot;https://www.continuum.io/downloads&quot;&gt;downloads&lt;/a&gt; page and got the Python 2.7 Mac OS X 64-Bit &lt;em&gt;command-line installer&lt;/em&gt;, so that we can install everything from Terminal.&lt;/p&gt;

&lt;p&gt;After downloading that, navigate to your Downloads directory (if you’re new to the Terminal, just open the Terminal application and type &lt;code class=&quot;highlighter-rouge&quot;&gt;cd $HOME/Downloads&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;While still in Terminal, enter&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;bash Anaconda2-2.5.0-MacOSX-x86_64.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Awesome, now you’ve downloaded and installed Anaconda.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1b-make-anaconda-your-default-python-installation&quot;&gt;(1.b) Make Anaconda your default python installation.&lt;/h3&gt;
&lt;p&gt;For data science, Anaconda rules. Ideally, when you’re in Terminal and you type &lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt;, you’d like for the Anaconda python installation to be the default python that starts running instead of what comes installed by default on a typical Macbook. Why? Well, using Anaconda we can just import NumPy, import any Scikit Learn funciton, import Matplotlib, etc.&lt;/p&gt;

&lt;p&gt;To see what I’m talking about, type this in Terminal:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;which python&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you get &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/bin/python2.7&lt;/code&gt;, you’re not using the Anaconda installation. To change this, you’ll need to change your &lt;strong&gt;bash_profile&lt;/strong&gt; so that the default path to the python installation in the Anaconda directory. If you don’t have a .bash_profile file in your home directory, do this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;touch &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bash_profile&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This just created that file. Next, open the .bash_profile page and add this line:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;export PATH=”~/anaconda/bin:$PATH”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Finally, you have to make your system update python path the with your new settings, so in Terminal type&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bash_profile&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-make-an-anaconda-virtual-environment&quot;&gt;(2) Make an Anaconda virtual environment&lt;/h3&gt;

&lt;p&gt;Anaconda has great &lt;a href=&quot;http://conda.pydata.org/docs/py2or3.html#create-a-python-2-7-environment&quot;&gt;documentation&lt;/a&gt; if you ever get lost using their tools, but otherwise they’re pretty easy to use. To create a virtual python 2.7 environment called “py27,” run this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;conda create -n py27 &lt;span class=&quot;nv&quot;&gt;python&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2.7 anaconda&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To enter this virtual environment, we use Conda’s &lt;code class=&quot;highlighter-rouge&quot;&gt;source activate&lt;/code&gt; function:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;activate py27&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If the environment is running properly, you should see &lt;code class=&quot;highlighter-rouge&quot;&gt;(py27)&lt;/code&gt; preceding the &lt;code class=&quot;highlighter-rouge&quot;&gt;$&lt;/code&gt; sign at the command prompt in Terminal. In this environment we have access to Anaconda’s python package installer, &lt;code class=&quot;highlighter-rouge&quot;&gt;conda install&lt;/code&gt;, so that we can install packages at will in this “bubble” without messing up dependencies (basically breaking python) in any other environment. Side note: if you want to exit this py27 environment, just enter &lt;code class=&quot;highlighter-rouge&quot;&gt;source deactivate&lt;/code&gt; in Terminal.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-update-packages&quot;&gt;(3) Update packages&lt;/h3&gt;
&lt;p&gt;Just to be safe, I updated all of my python packages while inside of my py27 environment. It’s ridiculously easy with Anaconda:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;conda update --all&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-install-opencv&quot;&gt;(4) Install OpenCV&lt;/h3&gt;
&lt;p&gt;With Anconda we can install python packages within a specific Conda environment using &lt;code class=&quot;highlighter-rouge&quot;&gt;conda install&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;pip&lt;/code&gt;, the typical python package management system.&lt;/p&gt;

&lt;p&gt;Next, I would normally suggest just typing &lt;code class=&quot;highlighter-rouge&quot;&gt;conda install opencv&lt;/code&gt; at the command prompt, but this (unsurprisingly) lead me to a package conflict with NumPy! Yep, the version of OpenCV that Conda installed relied on a specific release of the NumPy package that was actually in conflict with the one that was just updated in step (3). OK, to be honest, maybe I brought that upon myself with updating the packages the way I did. But, there’s a work around that functions with this latest update of NumPy: install OpenCV directly from the &lt;a href=&quot;http://www.menpo.org/&quot;&gt;Menpo project&lt;/a&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;conda install -c https://conda.binstar.org/menpo opencv&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5-fire-up-a-jupyter-notebook-and-test&quot;&gt;(5) Fire up a Jupyter notebook and test!&lt;/h3&gt;
&lt;p&gt;The Anaconda environment should now have everything we need to start analyzing images in a self-contained little Jupyter notebook. Test it out. First, launch a Jupyter notebook from the terminal:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;jupyter notebook&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next, see if everything is installed correctly; hopefully you’ll be able to run this sans errors:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FrankFineis/FrankFineis.github.io/master/images/opencv_import_ss.png&quot; alt=&quot;jupyter notebook&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If successful, you’ll be able to readily access OpenCV functions with the package prefix &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2&lt;/code&gt;!&lt;/p&gt;
</description>
        <pubDate>Sun, 13 Mar 2016 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/blog/2016/03/13/install_opencv.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2016/03/13/install_opencv.html</guid>
        
        
        <category>blog</category>
        
      </item>
    
      <item>
        <title>Example Case Interview: the Kaggle Competition. (Part 2)</title>
        <description>&lt;p&gt;After finishing &lt;a href=&quot;http://frankfineis.github.io/tutorials/2016/02/17/kaggle-pt1.html&quot;&gt;Part 1&lt;/a&gt; of this tutorial we have our data features - recall that we saved the TF-IDF transformed text data from the &lt;em&gt;names&lt;/em&gt; and &lt;em&gt;description/caption&lt;/em&gt; fields and country names we got from the Geonames API in the &lt;code class=&quot;highlighter-rouge&quot;&gt;./data&lt;/code&gt; directory - we’ll assemble our training and test data matrices. After that, we’ll train an &lt;code class=&quot;highlighter-rouge&quot;&gt;xgboost&lt;/code&gt; model comprised of trees and (briefly) tune a few hyperparameters.&lt;/p&gt;

&lt;p&gt;It’s important to note that there are two different ways to train and validate a model: we can use the functions supplied to us in the &lt;code class=&quot;highlighter-rouge&quot;&gt;xgboost&lt;/code&gt; package directly (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;xgb.train&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;xgb.cv&lt;/code&gt;), OR, we can use the &lt;code class=&quot;highlighter-rouge&quot;&gt;caret&lt;/code&gt; package. I’ll illustrate both, but I’ll use &lt;code class=&quot;highlighter-rouge&quot;&gt;caret&lt;/code&gt; to tune the model that will be used for making predictions on &lt;code class=&quot;highlighter-rouge&quot;&gt;test.csv&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Go ahead and navigate to the &lt;a href=&quot;https://github.com/fineiskid/photo_kaggle/blob/master/scripts/run_classifier.R&quot;&gt;run_classifier.R&lt;/a&gt; script to follow along!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-the-assemble_data-function&quot;&gt;(1) The &lt;code class=&quot;highlighter-rouge&quot;&gt;assemble_data&lt;/code&gt; function&lt;/h3&gt;

&lt;p&gt;This function will load and format either your training or test data (from &lt;code class=&quot;highlighter-rouge&quot;&gt;train.csv&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;test.csv&lt;/code&gt; accordingly). Namely, this data pre-processing function will do the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Assembling training set&lt;/em&gt;: we change the &lt;em&gt;good&lt;/em&gt; vector in &lt;code class=&quot;highlighter-rouge&quot;&gt;train.csv&lt;/code&gt; to zeros and ones for use with Xgboost’s function model training function, &lt;code class=&quot;highlighter-rouge&quot;&gt;xgboost.train&lt;/code&gt;, append each of the the TF-IDF features from the &lt;em&gt;name&lt;/em&gt; and &lt;em&gt;description/caption&lt;/em&gt; text columns, append a &lt;a href=&quot;https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science&quot;&gt;one-hot encoding&lt;/a&gt; of the country names data, remove rows whose countries are missing (there are only about 150 of these cases), and return the training data and target vector with R’s &lt;code class=&quot;highlighter-rouge&quot;&gt;as.matrix&lt;/code&gt; function. One-hot encoding of the country names will make one binary vector for each country name, and that vector will have a 1 if that row’s photo album came from the corresponding country. This is accomplished with the line&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country_one_hot&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model.matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Assembling the test set&lt;/em&gt;: Again, we append text and one-hot encoded country features, only now we can ignore the target vector because we’re obviously missing &lt;em&gt;good&lt;/em&gt;/&lt;em&gt;bad&lt;/em&gt; photo albumn classifications. Also, instead of removing rows with missing country names, we’ll impute the most common country name (the USA) because we don’t have the luxury of just excluding albums from the test set. This imputation happens on the line&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which.max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
Once you’re familiar with the &lt;code class=&quot;highlighter-rouge&quot;&gt;assemble_data&lt;/code&gt; function, notice that the lines immediately after defining &lt;code class=&quot;highlighter-rouge&quot;&gt;assemble_data&lt;/code&gt; are calling that function to gather our training set and training target vector:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assemble_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2a-train-a-model-with-randomly-chosen-parameters-optional&quot;&gt;(2.a) Train a model with randomly chosen parameters (optional)&lt;/h3&gt;

&lt;p&gt;If you want to use the model training, validation, and prediction functions supplied in the Xgboost package, that’s great, just be aware that you should supply each of &lt;code class=&quot;highlighter-rouge&quot;&gt;xgb.train&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;xgb.cv&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;xgb::predict&lt;/code&gt; with special data structures created with &lt;code class=&quot;highlighter-rouge&quot;&gt;xgb.DMatrix&lt;/code&gt; instead of regular old matrices or R data.frames. First, we split up the &lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt; data into a training set and a validation set. I know, it’s confusing terminology - we’re taking 80% of the training data and calling &lt;em&gt;this&lt;/em&gt; the training set and the remaining 20% the &lt;em&gt;validation&lt;/em&gt; set. Anyway, the next chunk of code will fit a xgboosted tree model with a set of hyperparameters I pulled out of nowhere. Note that by using the &lt;code class=&quot;highlighter-rouge&quot;&gt;watchlist&lt;/code&gt; setting, the training and validation set performance will print to the screen each time a new one of the &lt;code class=&quot;highlighter-rouge&quot;&gt;nround&lt;/code&gt; small trees is appended to the aggregate model, but I’m saving this output to a text file via R’s &lt;code class=&quot;highlighter-rouge&quot;&gt;sink&lt;/code&gt; function. Finally, we can estimate our accuracy on &lt;code class=&quot;highlighter-rouge&quot;&gt;test.csv&lt;/code&gt; by using our prediction error on the validation set as a proxy:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;trainIndex&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataPartition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                                         &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#fraction of training data kept as 'training' data
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                                         &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#hyperparameters
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max.depth&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#How deep each weak learner (small tree) can get. Will control overfitting.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#eta is the learning rate, which also has to do with regularization. 1 -&amp;gt;&amp;gt; no regularization. 0 &amp;lt; eta &amp;lt;= 1.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nround&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;150&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#The number of passes over training data. This is the number of trees we're ensembling.
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train.DMat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb.DMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.DMat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb.DMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#Fit boosted model with our random parameters. Save output that would otherwise print to console.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sink&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;./data/watchlist_output.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bst&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb.train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train.DMat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;watchlist&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train.DMat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.DMat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max.depth&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max.depth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nthread&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nround&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nround&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objective&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;binary:logistic&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval_metric&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;logloss&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sink&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.preds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.DMat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#Xgboost::predict returns class probabilities, not class labels!
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.class_preds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.preds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.accuracy&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;as.numeric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.class_preds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#~78% valid accuracy
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sprintf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Accuracy on validation set: %f&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Before moving on, one cool feature of boosted tree models is that we can get &lt;strong&gt;variable importances&lt;/strong&gt; - a variable’s importance to the overall model is an average of the improvement in accuracy gained from all of the small trees every time that feature was used to split up a node in a tree. Viewing the variable importances might help to verify that your model makes sense, and it wouldn’t hurt to include it in a code/case interview to demonstrate that you can put words behind the model you’ve built.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;importance_matrix&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb.importance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_names&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colnames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;importance_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;importance_matrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;horiz&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;names.arg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;importance_matrix&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Estimated top 6 features by accuracy gain&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cex.names&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/fineiskid/photo_kaggle/master/output/feature_importance.png&quot; class=&quot;inline&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2b-tune-hyperparameters-with-carettrain&quot;&gt;(2.b) Tune hyperparameters with &lt;code class=&quot;highlighter-rouge&quot;&gt;caret::train&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Since we really have little insight into whether we should use a small &lt;code class=&quot;highlighter-rouge&quot;&gt;max_depth&lt;/code&gt; or a large one, let all of our features be available when constructing a weak learner (i.e. how to set &lt;code class=&quot;highlighter-rouge&quot;&gt;colsample_bytree&lt;/code&gt;), etc… We should set up a grid of parameters, cross validate a model for every combination of parameters in the grid, and pick the best one! This is a very computationally expensive way to pick a model, and note that it’s still not even very refined when there are big discrete gaps in the hyperparameter values present in the grid. Still, we might be able to eek out better performance with a high-level gridsearch. Use R’s convenient &lt;code class=&quot;highlighter-rouge&quot;&gt;expand.grid&lt;/code&gt; function to make a data.frame with every combination of hyperparameters you’re interested in; use &lt;code class=&quot;highlighter-rouge&quot;&gt;caret::trainControl&lt;/code&gt; to specify the type of validation you’re interested in (we’ll do 5-fold cross validation); and use &lt;code class=&quot;highlighter-rouge&quot;&gt;caret::train&lt;/code&gt; to search over the grid of hyperparameter combinations to find the model that minimizes log-loss. We can only use &lt;code class=&quot;highlighter-rouge&quot;&gt;caret&lt;/code&gt; because it recently began supporting &lt;code class=&quot;highlighter-rouge&quot;&gt;model = 'xgbTree'&lt;/code&gt; (caret supports hundreds of different models, actually), and it conveniently supports log-loss as an optimization metric too!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;xgb_grid&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand.grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrounds&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eta&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;colsample_bytree&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                        &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_child_weight&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tr_control&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainControl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classProbs&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allowParallel&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summaryFunction&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnLogLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#Using summaryFunction = summaryFunction will use ROC (i.e. AUC) to select optimal model. We want log-loss.
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verboseIter&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_cv1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;good&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;bad&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Target vector should be non-numeric factors to identify our task as classification, not regression.
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tuneGrid&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#Which hyperparameters we'll test.
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trControl&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tr_control&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Specify how cross validation should go down.
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;xgbTree&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;logLoss&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Convenient.
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;                       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximize&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;We&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;want&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;after&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;A couple of points regarding &lt;code class=&quot;highlighter-rouge&quot;&gt;caret&lt;/code&gt;’s functionality versus &lt;code class=&quot;highlighter-rouge&quot;&gt;xgboost&lt;/code&gt;’s: first, you’ll need to make &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt; a factor vector instead of a numeric vector so that &lt;code class=&quot;highlighter-rouge&quot;&gt;caret::train&lt;/code&gt; knows that you want to perform a classification task instead of a regression task. Also, with &lt;code class=&quot;highlighter-rouge&quot;&gt;caret&lt;/code&gt; we can return to using matrices and data.frames instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;xgb.DMatrix&lt;/code&gt;’s. Finally - this gridsearch will take a &lt;strong&gt;long time&lt;/strong&gt; (at least 3 hours on a Macbook with an i7 core processor).&lt;/p&gt;

&lt;h3 id=&quot;3-visualize-performance-with-an-roc-curve-optional&quot;&gt;(3) Visualize performance with an ROC curve (optional):&lt;/h3&gt;
&lt;p&gt;Receiver operating characteristics (ROC’s) are confusing, so I suggest reading the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;Wikipedia&lt;/a&gt; article about them. Maybe I’ll make a post about it soon. Basically, they just let you compare the performance of different classification models. A popular one called “area under curve” the false positive rate (i.e. rate at which we call a bad photo album good) versus the true positive rate (i.e. rate at which we call good albums good). When the false positive rate (FPR) is zero, we’ve just called every album ‘bad’ so the true positive rate (TPR) is 0%. Similarly, when we call every album good, the FPR is 100% and the TPR 100%. The best model ever would have 0% FPR and 100% TPR, so an “area under the curve” of 1. I suggest reading &lt;a href=&quot;http://blog.yhat.com/posts/roc-curves.html&quot;&gt;this&lt;/a&gt; great intro link about AUC and ROC curves. Anyway, you need to obtain class probabilities from &lt;code class=&quot;highlighter-rouge&quot;&gt;caret::train&lt;/code&gt; &lt;strong&gt;or&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;xgb.predict&lt;/code&gt; in order to build an ROC curve.&lt;/p&gt;

&lt;p&gt;My code in &lt;a href=&quot;https://github.com/fineiskid/photo_kaggle/blob/master/scripts/run_classifier.R&quot;&gt;run_classifier.R&lt;/a&gt; will let you build the ROC curve by hand as well as leverage the R packages &lt;code class=&quot;highlighter-rouge&quot;&gt;pROC&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ROCR&lt;/code&gt; for simplification:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;auc_est&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pROC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.preds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;good&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#0.8638 on the validation set, not bad...
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rocr_pred&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid.preds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;good&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rocr_perf&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;performance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rocr_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measure&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;tpr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x.measure&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;fpr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#this is of class &quot;performance,&quot; it's not a list
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auc_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;fpr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rocr_perf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x.values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;tpr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rocr_perf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y.values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/fineiskid/photo_kaggle/master/output/roc_curve.png&quot; class=&quot;inline&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-assemble-test-data-and-gather-predictions&quot;&gt;(4) Assemble test data and gather predictions&lt;/h3&gt;

&lt;p&gt;Just like in step (1), use &lt;code class=&quot;highlighter-rouge&quot;&gt;assemble_data&lt;/code&gt; to construct the test set with all of the new features we have. If you look in the &lt;code class=&quot;highlighter-rouge&quot;&gt;example_entry.csv&lt;/code&gt; file that the Kaggle administrator supplied, they want for the output to have two columns, &lt;em&gt;id&lt;/em&gt; and &lt;em&gt;good&lt;/em&gt;, where &lt;em&gt;good&lt;/em&gt; is binary 0/1. Since &lt;code class=&quot;highlighter-rouge&quot;&gt;caret::predict(model, newdata, type = &quot;raw&quot;)&lt;/code&gt; returns factor variables representing class membership, just use R’s super-convenient &lt;code class=&quot;highlighter-rouge&quot;&gt;ifelse&lt;/code&gt; function to change the results to zeros and ones. Save the data, and submit!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assemble_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_or_test&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;test&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_file&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;./data/test.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name_tfidf_file&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;./data/test_name_tfidf.rds&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;desc_caption_tfidf_file&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;./data/test_desc_caption_tfidf.rds&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;country_file&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;./data/aggregate_test_countries.RDS&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_classes&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xgb_cv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;raw&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;good&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#Save and then submit predictions!
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write.csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read.csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;./data/test.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;good&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
          &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;./output/test_predictions.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row.names&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;optional-run-this-bad-boy-yourself&quot;&gt;(Optional) run this bad boy yourself&lt;/h3&gt;

&lt;p&gt;From your command line, navigate to the &lt;code class=&quot;highlighter-rouge&quot;&gt;photo_kaggle&lt;/code&gt; directory and run &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;gt; Rscript ./scripts/run_classifier.R&lt;/code&gt;. This will build an xgboost model and save predictions from the &lt;code class=&quot;highlighter-rouge&quot;&gt;test.csv&lt;/code&gt; file provided to us on Kaggle. From here it’s easy to add and tune more hyperparameters, add some creative features, delete useless ones, etc. Xgboost is really powerful, have fun!&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Mar 2016 09:28:13 -0600</pubDate>
        <link>http://localhost:4000/tutorials/data/2016/03/08/kaggle-pt2.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/data/2016/03/08/kaggle-pt2.html</guid>
        
        
        <category>tutorials</category>
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>Example Case Interview: the Kaggle Competition. (Part 1)</title>
        <description>&lt;p&gt;Alright, so you’re an aspiring data scientist, say a graduate student in a STEM field trying to get into private industry, and congratulations, you’ve made it to the case study round! What do I mean by &lt;em&gt;case study&lt;/em&gt;? Ah, I mean a timed test with a &lt;em&gt;training set&lt;/em&gt; and a &lt;em&gt;test set&lt;/em&gt;. You’ve been instructed to build a model that will give predicted values for the observations contained in the test set, and then your (hopefully) future employer will compare your values with the test set’s true values. Few things get more meritocratic than that!&lt;/p&gt;

&lt;h4 id=&quot;general-steps-involved-in-a-data-science-coding-interview&quot;&gt;General steps involved in a data science coding interview:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Load your data. Poke around.&lt;/li&gt;
  &lt;li&gt;Decide on an appropriate mathematical/machine learning model you’d like to use.&lt;/li&gt;
  &lt;li&gt;Create a data processing pipeline for both your training and test data sets.
    &lt;ul&gt;
      &lt;li&gt;Pick features you’d like to try in your model. A &lt;strong&gt;feature&lt;/strong&gt; is generally a column within your dataset fed into your machine learning model.&lt;/li&gt;
      &lt;li&gt;If applicable, standardize or normalize various features.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Tune the model’s hyperparameters through validation.&lt;/li&gt;
  &lt;li&gt;Fit model with decided upon hyperparameters with all of your training data.&lt;/li&gt;
  &lt;li&gt;Get predictions for the processed test data set.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;example-case-interview-the-kaggle-photo-album-classification-project&quot;&gt;Example case interview: the Kaggle photo album classification project&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt; is a data science competition website where individuals (and teams) compete, sometimes for big bucks, on data science projects hosted by big name companies (Deloitte and NASA, just to name a few) who want/need models and solutions to said data science projects. The projects run the gamut in difficulty, and I’ve picked an easier one that serves as a good example for what you might expect for a data science case interview: the &lt;a href=&quot;https://www.kaggle.com/c/PhotoQualityPrediction&quot;&gt;photo album quality prediction&lt;/a&gt; project. You can track all of my relevant code and data &lt;a href=&quot;https://github.com/fineiskid/photo_kaggle&quot;&gt;here&lt;/a&gt; in this Github repo. The main idea is that we have data corresponding to photo albums that have been uploaded by users, and somehow they were labeled “good” or “bad.” In the training set we have information about each photo album like number of photos, size of each photo, etc., and we seek to make a good/bad classification for a set of albums that have not been labeled, this latter set being the test set.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-gist&quot;&gt;THE GIST:&lt;/h2&gt;
&lt;p&gt;Like I said, we’ll need to process the training data, seek out new features, pick a model, train and tune it, process the test data, and get our predictions on the test data. I’m going to do this using &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt;, and since there’s been a lot of buzz about &lt;strong&gt;extreme gradient boosting&lt;/strong&gt; over the last year or two, and the &lt;a href=&quot;https://www.youtube.com/watch?v=Og7CGAfSr_Y&quot;&gt;xgboost library&lt;/a&gt; that implements the technique has won at least one Kaggle competition (probably more, I didn’t feel like researching it further), we’ll use xgboost to build a model/get predictions.&lt;/p&gt;

&lt;p&gt;I’m not going to get into the nitty gritty of boosted trees and gradient boosting, but from a high level, you can think of it as a weighted sum of small decision trees. The size of the weighted sum (i.e. the number of decision trees) is a hyper-parameter that we’ll tune. Each additional decision tree itself comes from fitting the residuals between the outcome and the existing weighted sum of small residual trees. Xgboost by default builds this type of boosted tree model for classification purposes, but it can also be used for continuous, regression learning purposes by setting &lt;code class=&quot;highlighter-rouge&quot;&gt;objective = &quot;reg:linear&quot;&lt;/code&gt; within the parameter list submitted to &lt;code class=&quot;highlighter-rouge&quot;&gt;xbg.train&lt;/code&gt; (more on this to come). I’d Google it, but let’s move on to data preprocessing first.&lt;/p&gt;

&lt;p&gt;When we make a model using the training data, we’ll need to make sure that we treat the test data in exactly the same way we did the training data to get the same results. For example, if we scale one feature in the training set, we have to scale the corresponding feature in the test set. Staying organized is the number one priority!&lt;/p&gt;

&lt;p&gt;We will be evaluated according to the &lt;strong&gt;log-loss&lt;/strong&gt; the host will calculate from our vector of predictions on the albums put forth in &lt;code class=&quot;highlighter-rouge&quot;&gt;test.csv&lt;/code&gt;. We could theoretically evaluate log-loss by hand, it’s given by&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.sciweavers.org/tex2img.php?eq=%24%24-%5Csum%20%28y_%7Bi%7Dlog%28y_%7Bpred%7D%29%20%2B%20%281-y_%7Bi%7D%29log%281-y_%7Bpred%7D%29%29%24%24&amp;amp;bc=White&amp;amp;fc=Black&amp;amp;im=jpg&amp;amp;fs=12&amp;amp;ff=arev&amp;amp;edit=0 align=&amp;quot;center&amp;quot; border=&amp;quot;0&amp;quot; alt=&amp;quot;$$-\sum (y_{i}log(y_{pred}) + (1-y_{i})log(1-y_{pred}))$$&amp;quot; width=&amp;quot;339&amp;quot; height=&amp;quot;22&amp;quot; /&amp;gt;&quot; alt=&quot;equation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fortunately, the &lt;code class=&quot;highlighter-rouge&quot;&gt;xgboost&lt;/code&gt; library will allow us to specify log-loss as the loss metric we want to minimize.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-our-data-looks-like&quot;&gt;What our data looks like:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;id&lt;/code&gt;: index of the photo album. This is pretty useless towards training a predictive model, so this will be removed from training and test sets.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;latitude&lt;/code&gt;: integer-rounded latitude coordinates of geo-tag of the photo album.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;longitude&lt;/code&gt;: integer-rounded longitude coordinates of geo-tag of the photo album.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;width&lt;/code&gt;: The width of the images in the album, in pixels&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;height&lt;/code&gt;: The height of the images in the album, in pixels&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;size&lt;/code&gt;: The number of photos in the album&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt;: Common words in the name of the album&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;description&lt;/code&gt;: The common words in the description of the album&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;caption&lt;/code&gt;: The common words in all the captions of the photos within the album&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;good&lt;/code&gt;: 1 means the human reviewer liked the album, 0 means they didn’t (we’re trying to predict this value). Note that this column is only present in &lt;code class=&quot;highlighter-rouge&quot;&gt;training.csv&lt;/code&gt; and not in &lt;code class=&quot;highlighter-rouge&quot;&gt;test.csv&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that the “words” have ben anonymized into integers. A word is present in the dataset if it has occured “more than 20 times in different albums,” according to the Kaggle competition’s host - this does not mean that the words in our dataset each occur at least 20 times (few of them do, actually).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;extra-features&quot;&gt;Extra features:&lt;/h2&gt;
&lt;p&gt;The main features that come with the training/test data are integer-rounded latitude/longitude coordinates, height/width of the photos used in albums being studied, the number of photos being studied (&lt;code class=&quot;highlighter-rouge&quot;&gt;size&lt;/code&gt;), and anonymized text from the album name, album description, and album caption.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Area&lt;/em&gt;: area is a nonlinear feature (it’s multiplicative, duh) that might help us eek-out some extra performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Country&lt;/em&gt;: country name may help; perhaps well-liked albums are more likely in exotic locales. While latitude/longitude might get us most of the way there especially with a tree-based algorithm like xgboost, this might help when coordinates are clustered together at the integer-level, but actual nation varies. I used &lt;a href=&quot;http://www.geonames.org/&quot;&gt;Geonames&lt;/a&gt; to gather country names based on the longitude/latitude integers supplied (see &lt;a href=&quot;https://github.com/fineiskid/photo_kaggle/blob/master/src/geographic_features.R&quot;&gt;geographic_features.R&lt;/a&gt; script) and saved a vector of names for each of the training and test sets. There’s a great R package aptly titled &lt;code class=&quot;highlighter-rouge&quot;&gt;geonames&lt;/code&gt; that handles all of the URL requests for you, and my script should walk you through the country name acquisition process pretty smoothly. When it’s time to build the boosted model, we simply load &lt;code class=&quot;highlighter-rouge&quot;&gt;aggregate_train_countries.RDS&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;aggregate_test_countries.RDS&lt;/code&gt; with R’s &lt;code class=&quot;highlighter-rouge&quot;&gt;readRDS&lt;/code&gt; function and attach them to our feature matrices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Text frequency inverse document frequency data&lt;/em&gt;: if we treat each album’s name, caption, or description as a “document,” then we can get text features by measuring each word’s importance to the “document.” Check out the &lt;a href=&quot;https://github.com/fineiskid/photo_kaggle/blob/master/src/text_features.R&quot;&gt;text_features.R&lt;/a&gt; script to see exactly how I’ve computed the TF-IDF features for the training and test sets. For a given word in a “document,” that word’s TF-IDF value will increase if the word occurs frequently in the document and decrease if the word is common across many different documents. I’ve built separate TF-IDF matrices for the &lt;code class=&quot;highlighter-rouge&quot;&gt;names&lt;/code&gt; feature and a separate, combined matrix for both the &lt;code class=&quot;highlighter-rouge&quot;&gt;description&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;caption&lt;/code&gt; features (I just mashed those features together as I don’t know the difference between a photo album caption and a description). See &lt;code class=&quot;highlighter-rouge&quot;&gt;text_features.R&lt;/code&gt; for the calculations. Again, we generate the TF-IDF features for each of the training and test sets and load them when necessary. An important note about TF-IDF features: because we can’t assume that words present in the test set will necessarily be contained within the vocabulary of the training set, we have to designate the words in the test set not seen in the training set as having a TF-IDF value of zero.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-left-to-do&quot;&gt;What’s left to do?&lt;/h2&gt;
&lt;p&gt;As of now have chosen a statistical model, checked out our data, and extracted some more features. Next, we’ll have to train and tune an xgboost model. There are a &lt;strong&gt;bunch&lt;/strong&gt; of parameters we can tune to try to minimize the log-loss, and we might look at some of these:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;max.depth&lt;/code&gt; : how many levels each small trees in our model, which is a sum of small trees, will be allowed to have&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gamma&lt;/code&gt; : when picking a new small tree to add to our model, this is the minimum reduction in loss required to allow a node on a tree to be split further into two child nodes&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;colsample_bytree&lt;/code&gt;: in an attempt to de-correlate the small trees we’re adding to our model, for each tree we’ll choose a fraction of columns to pick from the training data to build said tree (I think this is similar to &lt;code class=&quot;highlighter-rouge&quot;&gt;max_features&lt;/code&gt; in Python’s Random Forest classifier in Sklearn)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;num_round&lt;/code&gt;: How many small trees comprise the model? This also represents the number of times we sweep over the training data, as we get one small tree per sweep.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While in a Kaggle competition or a data science code interview you won’t be able to know exactly how erroneous your predictions are on the test set, we can use cross-validation to get an estimate of test error for each set of parameters we’ll test. Picking the set of parameters resulting in the least amount of cross-validated test error, we use those parameters to run &lt;code class=&quot;highlighter-rouge&quot;&gt;xgboost::predict()&lt;/code&gt; and submit our predictions!&lt;/p&gt;

</description>
        <pubDate>Tue, 16 Feb 2016 23:28:13 -0600</pubDate>
        <link>http://localhost:4000/tutorials/2016/02/16/kaggle-pt1.html</link>
        <guid isPermaLink="true">http://localhost:4000/tutorials/2016/02/16/kaggle-pt1.html</guid>
        
        
        <category>tutorials</category>
        
      </item>
    
      <item>
        <title>Data science: an elusive concept? Not really.</title>
        <description>&lt;p&gt;&lt;em&gt;Predictive analytics&lt;/em&gt;, &lt;em&gt;deep learning&lt;/em&gt;, &lt;em&gt;big data&lt;/em&gt;: they’re all buzzwords that surround this term we’ve settled on called &lt;em&gt;data science&lt;/em&gt;. Data science has been called the crossroads of traditional computer science, statistics, and computer programming. To me, data science too often gets funneled into a simple application of a machine learning technique for binary classification purposes. If that means nothing to you, it just means classifying objects as a 0 (negative response) or 1 (positive response). That said, binary classification tasks are a huge facet of predictive modeling! Data science can encompass everything from a simple linear regression model, to computer vision, linguistic analysis, database structure, and even &lt;a href=&quot;http://www.anishathalye.com/2015/12/19/an-ai-that-can-mimic-any-artist/&quot;&gt;Van Gogh&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FrankFineis/FrankFineis.github.io/master/images/van_gogh_screenshot.png&quot; class=&quot;inline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Yes&lt;/strong&gt;, art is quantifiable according to convolutional neural networks.&lt;/p&gt;

&lt;p&gt;Overall, &lt;em&gt;data science&lt;/em&gt; is exactly what it sounds like - experimenting and trying to draw conclusions from datasets. While there are hundreds, if not thousands of different specific fields contained within data science such as regression modeling, predictive modeling, high-performance computing cluster management, neural nets (don’t get me started on neural nets), I guarantee that there is a huge body of literature and guides for the flavor of data science you’re interested in. My goals for the aspiring data scientist - have fun, be creative, experiment, and try to learn Git! Data scientists these days (data science as we know it really emerged hundreds of years ago with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Actuary#Need_for_insurance&quot;&gt;rise of amateur actuaries&lt;/a&gt;) all will mostly have a foundation in R and/or Python, basic statistical/probability knowledge, and from there they will try to specialize in particular fields most interesting or useful to them.&lt;/p&gt;

&lt;!-- cool: include links at the end of your post... --&gt;

</description>
        <pubDate>Tue, 09 Feb 2016 09:28:13 -0600</pubDate>
        <link>http://localhost:4000/blog/data/2016/02/09/data-science-an-elusive-concept.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/data/2016/02/09/data-science-an-elusive-concept.html</guid>
        
        
        <category>blog</category>
        
        <category>data</category>
        
      </item>
    
  </channel>
</rss>
